<div align="center">

# ğŸµ LSTM Frequency Filter

### *Deep Learning Meets Signal Processing*

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

**Train an LSTM neural network to intelligently filter individual frequency components from mixed signals**

[Overview](#-project-overview) â€¢ [Dataset](#-dataset-creation) â€¢ [Model](#-model-architecture) â€¢ [Results](#-results) â€¢ [Visualizations](#-visualizations)

---

</div>

## ğŸ¯ Project Overview

This project demonstrates the power of **Long Short-Term Memory (LSTM)** networks for signal processing tasks. We train a neural network to act as an intelligent frequency filter that can:

- ğŸ¼ **Decompose** complex multi-frequency signals into individual components
- ğŸ›ï¸ **Select** specific frequencies using one-hot encoded selectors
- ğŸ“Š **Achieve** 94.8% accuracy (RÂ² score) in frequency extraction
- âš¡ **Process** signals in real-time with learned temporal patterns

<div align="center">

### The Complete Pipeline

![Complete Overview](visualizations/00_complete_overview.png)

*One-page visualization showing dataset, model architecture, training progress, and results*

</div>

---

## ğŸ¼ The Frequency Challenge

### Problem Statement

Imagine you have a mixed audio signal containing multiple musical notes playing simultaneously. Can a neural network learn to isolate just one specific note based on your selection? **Yes!**

Given a combined signal `S(x)` composed of four phase-shifted sine wave frequencies:

```math
S(x) = sin(2Ï€Â·fâ‚Â·x + Î¸â‚) + sin(2Ï€Â·fâ‚‚Â·x + Î¸â‚‚) + sin(2Ï€Â·fâ‚ƒÂ·x + Î¸â‚ƒ) + sin(2Ï€Â·fâ‚„Â·x + Î¸â‚„)
```

Our LSTM model learns to extract a specific frequency component `fáµ¢(x)` from `S(x)` based on a one-hot selector vector `c = [câ‚, câ‚‚, câ‚ƒ, câ‚„]`.

### ğŸ“» Our Four Phase-Shifted Frequencies

We chose four harmonically distinct frequencies with different phase shifts to create a realistic signal processing challenge:

| Frequency | Hz | Phase Î¸ (rad) | Phase Î¸ (degrees) | Period (s) | Cycles in 20s |
|-----------|-----|---------------|-------------------|-----------|---------------|
| **fâ‚** | 1.0 | 0.000 | 0Â° | 1.000 | 20 |
| **fâ‚‚** | 3.0 | 0.785 (Ï€/4) | 45Â° | 0.333 | 60 |
| **fâ‚ƒ** | 5.0 | 1.571 (Ï€/2) | 90Â° | 0.200 | 100 |
| **fâ‚„** | 7.0 | 2.356 (3Ï€/4) | 135Â° | 0.143 | 140 |

**Why these frequencies with phase shifts?**
- âœ… Well-separated in frequency domain (easy to visualize in FFT)
- âœ… Different phases create realistic signal mixing scenarios
- âœ… Phase shifts make the filtering task more challenging
- âœ… Simulates real-world signals where components don't start in phase
- âœ… Tests the model's ability to handle temporal offsets
- âœ… Span different temporal scales (from slow 1 Hz to faster 7 Hz)

<div align="center">

### Individual Frequencies in Time Domain

![Time Domain Signals](visualizations/01_time_domain_signals.png)

*Each frequency has its own characteristic oscillation pattern with unique phase offset. Notice how f3 (90Â°) starts at maximum while f1 (0Â°) starts at zero. When combined, they create complex interference patterns.*

</div>

<div align="center">

### Frequency Spectrum Analysis (FFT)

![Frequency Domain](visualizations/02_frequency_domain_fft.png)

*Fourier transform reveals the four distinct frequency peaks. The combined signal contains all four components.*

</div>

---

## ğŸ“Š Dataset Creation

### Signal Generation Process

We generate a rich, high-resolution dataset that captures the full dynamics of our multi-frequency system:

#### 1ï¸âƒ£ **Sampling Strategy**
- **Total samples**: 10,000 data points
- **Time interval**: [0, 20] seconds
- **Sampling rate**: 500 Hz (500 samples/second)
- **Duration**: Long enough to capture 20 cycles of the slowest frequency (fâ‚)

#### 2ï¸âƒ£ **Mathematical Foundation with Phase Shifts**

For each frequency component `fáµ¢`, we compute with phase shift `Î¸áµ¢`:

```python
fâ‚(x) = sin(2Ï€ Â· 1.0 Â· x + 0.000)      # 1 Hz, phase = 0Â° (reference)
fâ‚‚(x) = sin(2Ï€ Â· 3.0 Â· x + Ï€/4)        # 3 Hz, phase = 45Â°  
fâ‚ƒ(x) = sin(2Ï€ Â· 5.0 Â· x + Ï€/2)        # 5 Hz, phase = 90Â°
fâ‚„(x) = sin(2Ï€ Â· 7.0 Â· x + 3Ï€/4)       # 7 Hz, phase = 135Â°
```

Then combine them into the composite signal:

```python
S(x) = fâ‚(x) + fâ‚‚(x) + fâ‚ƒ(x) + fâ‚„(x)
```

**Phase Shift Impact:**
- At `x = 0`: fâ‚ starts at 0, fâ‚‚ at 0.707, fâ‚ƒ at 1.0, fâ‚„ at 0.707
- This creates a more complex initial condition: `S(0) = 2.414`
- Phase shifts make the model learn temporal relationships, not just frequency content

#### 3ï¸âƒ£ **Dataset Structure**

Our dataset is organized as a table with 10,000 rows:

| Sample | X value | fâ‚(x) | fâ‚‚(x) | fâ‚ƒ(x) | fâ‚„(x) | S(x) |
|--------|---------|-------|-------|-------|-------|------|
| 0 | 0.000 | 0.000 | 0.707 | 1.000 | 0.707 | 2.414 |
| 1 | 0.002 | 0.013 | 0.733 | 0.998 | 0.642 | 2.386 |
| 2 | 0.004 | 0.025 | 0.758 | 0.992 | 0.572 | 2.348 |
| ... | ... | ... | ... | ... | ... | ... |
| 9999 | 20.000 | 0.000 | 0.707 | 1.000 | 0.707 | 2.414 |

*Note: The phase shifts create different starting amplitudes for each frequency component.*

<div align="center">

### Signal Visualization

![Signal Overlay](visualizations/04_overlay_signals.png)

*All four phase-shifted frequencies overlaid with the combined signal. Notice how different phase offsets create unique interference patterns - fâ‚ƒ (90Â°, red) peaks when fâ‚ (0Â°, blue) crosses zero.*

![Spectrogram](visualizations/03_spectrogram.png)

*Time-frequency spectrogram showing constant frequency components over time. Phase shifts affect the temporal pattern but not the frequency content.*

</div>

#### 4ï¸âƒ£ **Creating Training Sequences**

To train the LSTM, we create **sequences** from the continuous signal:

- **Sequence length**: 50 time steps (window size)
- **Sliding window**: Stride of 1 (maximum overlap)
- **Total sequences**: 9,951 from the original signal
- **Training samples**: 39,800 (4 per sequence, one for each frequency)

Each training sample consists of:
- **Input**: Combined signal S(x) [50 timesteps] + One-hot selector [4 values]
- **Target**: Selected frequency fáµ¢(x) [50 timesteps]

#### 5ï¸âƒ£ **One-Hot Selector Encoding**

The selector tells the model which frequency to extract:

```python
câ‚ = [1, 0, 0, 0]  # "Extract fâ‚ (1 Hz) from the signal"
câ‚‚ = [0, 1, 0, 0]  # "Extract fâ‚‚ (3 Hz) from the signal"
câ‚ƒ = [0, 0, 1, 0]  # "Extract fâ‚ƒ (5 Hz) from the signal"
câ‚„ = [0, 0, 0, 1]  # "Extract fâ‚„ (7 Hz) from the signal"
```

<div align="center">

### Training Sample Structure

![Training Samples](visualizations/05_training_samples.png)

*Example training pairs: Input signal with selector â†’ Target frequency output*

![Model I/O Structure](visualizations/06_model_io_structure.png)

*Detailed view of how input features (signal + selector) map to output (filtered frequency)*

</div>

#### 6ï¸âƒ£ **Data Split**

We split the dataset to ensure robust evaluation:

| Split | Sequences | Percentage | Purpose |
|-------|-----------|------------|---------|
| **Training** | 31,840 | 80% | Model learning |
| **Validation** | 3,980 | 10% | Hyperparameter tuning & early stopping |
| **Test** | 3,980 | 10% | Final performance evaluation |

**Statistical Properties:**
- Mean signal value: ~0 (centered)
- Amplitude range: [-4, +4] (sum of 4 unit sine waves)
- Standard deviation: 1.41 (âˆš2, as expected for sum of independent signals)

---

## ğŸ§  Model Architecture

We chose **PyTorch** as our deep learning framework for its flexibility, excellent LSTM implementation, and strong community support.

### Network Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INPUT LAYER                       â”‚
â”‚  Shape: (batch, 50 timesteps, 5 features)          â”‚
â”‚  â€¢ 1 signal value: S(x)                            â”‚
â”‚  â€¢ 4 selector values: one-hot encoding             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LSTM LAYER 1                           â”‚
â”‚  â€¢ 128 hidden units                                 â”‚
â”‚  â€¢ Bidirectional: No (causal)                      â”‚
â”‚  â€¢ Dropout: 0.2 (between layers)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LSTM LAYER 2                           â”‚
â”‚  â€¢ 128 hidden units                                 â”‚
â”‚  â€¢ Captures higher-level temporal patterns         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DROPOUT LAYER                          â”‚
â”‚  â€¢ Rate: 0.2 (prevents overfitting)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FULLY CONNECTED LAYER                      â”‚
â”‚  â€¢ Maps 128 features â†’ 1 output                    â”‚
â”‚  â€¢ No activation (regression task)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                OUTPUT LAYER                         â”‚
â”‚  Shape: (batch, 50 timesteps, 1)                   â”‚
â”‚  Value: Filtered frequency signal                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total Parameters**: 201,345 (all trainable)

### Hyperparameters & Design Choices

| Component | Value | Rationale |
|-----------|-------|-----------|
| **Hidden Size** | 128 | Sufficient capacity for 4 frequency patterns without overfitting |
| **Num Layers** | 2 | Captures both local oscillations and longer-term patterns |
| **Dropout** | 0.2 | Optimal balance between regularization and performance |
| **Batch Size** | 64 | Good GPU utilization while maintaining stable gradients |
| **Learning Rate** | 0.001 | Adam default, proven effective for LSTMs |
| **Weight Decay** | 1e-5 | L2 regularization to prevent overfitting |
| **Loss Function** | MSE | Standard for regression; penalizes amplitude errors quadratically |
| **Optimizer** | Adam | Adaptive learning rates handle varying gradient magnitudes |
| **Epochs** | 50 | With early stopping (patience=15) |

### Why LSTM?

LSTMs excel at this task because they:
- âœ… **Remember long-term dependencies** (crucial for low frequencies)
- âœ… **Handle variable-length sequences** naturally
- âœ… **Learn temporal patterns** in the oscillations
- âœ… **Avoid vanishing gradients** (unlike vanilla RNNs)
- âœ… **Process time series** in their natural sequential form

---

## ğŸ“ˆ Training Process

<div align="center">

### Training Progress

![Training Loss](visualizations/07_training_loss.png)

*Training and validation loss curves showing smooth convergence. Note the log scale.*

</div>

### Training Characteristics

- **Convergence**: Smooth decrease in both training and validation loss
- **Best Epoch**: 47 (validation loss: 0.0251)
- **Final Training Loss**: 0.0332
- **Final Validation Loss**: 0.0296
- **Training Time**: ~12 minutes on CPU (50 epochs Ã— 14 seconds/epoch)
- **Early Stopping**: Not triggered (model continued improving)

### Optimization Details

1. **Gradient Clipping**: Max norm of 1.0 (prevents exploding gradients)
2. **Learning Rate Scheduling**: ReduceLROnPlateau (factor=0.5, patience=5)
3. **Batch Processing**: 497 batches per epoch (31,840 samples Ã· 64 batch size)
4. **Validation Frequency**: Every epoch

---

## ğŸ† Results

### Overall Performance Metrics

<div align="center">

| Metric | Value | Interpretation |
|--------|-------|----------------|
| ğŸ¯ **RÂ² Score** | **0.945** | Model explains **94.5%** of variance |
| ğŸ“Š **Correlation** | **0.972** | Very strong linear relationship |
| ğŸ“‰ **RMSE** | **0.165** | Average error of Â±0.165 amplitude |
| ğŸ“ **MAE** | **0.068** | Median error is very low |
| ğŸ”¢ **MSE** | **0.027** | Low squared error |

</div>

### Per-Frequency Performance

| Frequency | Hz | Phase | MSE â†“ | RMSE â†“ | MAE â†“ | RÂ² Score â†‘ | Performance |
|-----------|-----|-------|-------|--------|-------|------------|-------------|
| **fâ‚** | 1.0 | 0Â° | 0.0149 | 0.122 | 0.054 | **0.970** | â­â­â­â­â­ Excellent |
| **fâ‚‚** | 3.0 | 45Â° | 0.0318 | 0.178 | 0.072 | **0.937** | â­â­â­â­ Very Good |
| **fâ‚ƒ** | 5.0 | 90Â° | 0.0370 | 0.192 | 0.076 | **0.926** | â­â­â­â­ Very Good |
| **fâ‚„** | 7.0 | 135Â° | 0.0247 | 0.157 | 0.067 | **0.951** | â­â­â­â­â­ Excellent |

<div align="center">

### Performance Comparison Across Frequencies

![Per-Frequency Metrics](visualizations/13_per_frequency_metrics.png)

*Bar charts comparing MSE, RMSE, MAE, and RÂ² scores for each frequency*

</div>

### Key Findings

âœ… **Excellent overall RÂ² of 0.945** - Model successfully handles phase-shifted frequencies  
âœ… **Strong correlation of 0.972** - Predictions closely match actual values  
âœ… **Low RMSE of 0.165** - Predictions typically within Â±0.165 amplitude units  
âœ… **Best on fâ‚ (1 Hz, 0Â°)** - Lowest frequency with reference phase is easiest to filter (RÂ² = 0.970)  
âœ… **Phase-invariant performance** - Model filters all frequencies with RÂ² > 0.92 regardless of phase  
âœ… **No obvious bias** - Errors are normally distributed around zero  
âœ… **Handles temporal offsets** - Successfully learned to extract frequencies despite different phase shifts

---

## ğŸ¨ Visualizations

Our project includes **14 comprehensive visualizations** that tell the complete story from data generation to model evaluation.

### 1ï¸âƒ£ Complete Project Overview

<div align="center">

![Complete Overview](visualizations/00_complete_overview.png)

*Single-page summary: Dataset statistics, model architecture, training curves, and results*

</div>

### 2ï¸âƒ£ Prediction Quality

<div align="center">

![Predictions vs Actual](visualizations/08_predictions_vs_actual.png)

*Sample predictions for each frequency showing excellent match between predicted and actual signals*

![Long Sequences](visualizations/12_long_sequence_predictions.png)

*Extended time series showing the model maintains accuracy over long sequences*

</div>

### 3ï¸âƒ£ Model Performance Analysis

<div align="center">

![Scatter Plot](visualizations/10_scatter_pred_vs_actual.png)

*Predicted vs Actual scatter plot showing RÂ²=0.945 - points cluster tightly around the perfect prediction line, even with phase-shifted inputs*

![Error Distribution](visualizations/09_error_distribution.png)

*Error distribution and Q-Q plot - errors are normally distributed, indicating unbiased predictions*

</div>

### 4ï¸âƒ£ Frequency Domain Analysis

<div align="center">

![Frequency Spectrum Comparison](visualizations/11_frequency_spectrum_comparison.png)

*FFT comparison showing the model accurately preserves frequency content in predictions*

</div>

### Full Visualization Catalog

| # | Name | Description |
|---|------|-------------|
| 00 | Complete Overview | One-page project summary |
| 01 | Time Domain Signals | Individual frequencies over time |
| 02 | Frequency Domain FFT | Fourier analysis of all components |
| 03 | Spectrogram | Time-frequency representation |
| 04 | Signal Overlay | All frequencies superimposed |
| 05 | Training Samples | Input/target pairs for each frequency |
| 06 | Model I/O Structure | Input features and output format |
| 07 | Training Loss | Training and validation curves |
| 08 | Predictions vs Actual | Sample predictions comparison |
| 09 | Error Distribution | Error histogram and normality check |
| 10 | Scatter Plot | Correlation visualization |
| 11 | Frequency Spectrum | FFT comparison pred vs actual |
| 12 | Long Sequences | Extended time series predictions |
| 13 | Per-Frequency Metrics | Comparative performance bars |

---

---

## ğŸš€ Usage

### Quick Start

Run the complete pipeline with a single script:

```bash
chmod +x run_all.sh
./run_all.sh
```

Or run each step individually:

### Step-by-Step Execution

#### 1ï¸âƒ£ Generate Dataset
```bash
python generate_dataset.py
```
- Creates 10,000 samples of 4 frequencies
- Generates combined signal S(x)
- Saves to `data/frequency_dataset.csv` and `data/frequency_data.npz`
- **Output**: Dataset files ready for training

#### 2ï¸âƒ£ Visualize Data
```bash
python visualize_data.py
```
- Creates time-domain and frequency-domain plots
- Generates spectrograms and overlays
- **Output**: 4 visualization files in `visualizations/`

#### 3ï¸âƒ£ Prepare Training Data
```bash
python prepare_training_data.py
```
- Creates sequences with sliding windows
- Adds one-hot selectors to each sequence
- Splits into train/val/test sets
- **Output**: `data/training_data.npz` with 39,800 sequences

#### 4ï¸âƒ£ Train Model
```bash
python train_model.py
```
- Trains LSTM for up to 50 epochs
- Implements early stopping
- Saves best model based on validation loss
- **Output**: `models/best_model.pth` and training history

#### 5ï¸âƒ£ Evaluate Model
```bash
python evaluate_model.py
```
- Tests model on held-out test set
- Calculates performance metrics
- Creates comprehensive visualizations
- **Output**: 6 evaluation visualizations + metrics

#### 6ï¸âƒ£ View Summary
```bash
python summary.py
```
- Displays complete project statistics
- Shows all metrics and achievements
- **Output**: Console summary of entire project

#### 7ï¸âƒ£ Create Overview
```bash
python create_overview.py
```
- Generates single-page overview visualization
- **Output**: `visualizations/00_complete_overview.png`

---

## ğŸ’» Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- ~500 MB disk space

### Install Dependencies

```bash
# Using pip
pip install -r requirements.txt

# Or install individually
pip install numpy pandas matplotlib scipy torch scikit-learn
```

### Requirements

```
numpy>=1.24.0      # Numerical computations
pandas>=2.0.0      # Data manipulation
matplotlib>=3.7.0  # Visualization
scipy>=1.10.0      # Signal processing (FFT, spectrograms)
torch>=2.0.0       # Deep learning framework
scikit-learn>=1.3.0 # Metrics and data splitting
```

---

## ğŸ“ Project Structure

```
lstm-frequency-filter/
â”‚
â”œâ”€â”€ ğŸ“Š data/                           # Generated datasets
â”‚   â”œâ”€â”€ frequency_dataset.csv         # Raw tabular data
â”‚   â”œâ”€â”€ frequency_data.npz            # Signals (x, f1-f4, S)
â”‚   â””â”€â”€ training_data.npz             # Train/val/test sequences
â”‚
â”œâ”€â”€ ğŸ§  models/                         # Trained models
â”‚   â”œâ”€â”€ best_model.pth                # Best model weights (201K params)
â”‚   â”œâ”€â”€ training_history.npz          # Loss curves data
â”‚   â””â”€â”€ evaluation_results.npz        # Test metrics
â”‚
â”œâ”€â”€ ğŸ¨ visualizations/                 # All plots (14 total)
â”‚   â”œâ”€â”€ 00_complete_overview.png      # â­ Project summary
â”‚   â”œâ”€â”€ 01_time_domain_signals.png    # Signal plots
â”‚   â”œâ”€â”€ 02_frequency_domain_fft.png   # FFT analysis
â”‚   â”œâ”€â”€ 03_spectrogram.png            # Time-frequency
â”‚   â”œâ”€â”€ 04_overlay_signals.png        # Combined view
â”‚   â”œâ”€â”€ 05_training_samples.png       # I/O examples
â”‚   â”œâ”€â”€ 06_model_io_structure.png     # Architecture
â”‚   â”œâ”€â”€ 07_training_loss.png          # Training curves
â”‚   â”œâ”€â”€ 08_predictions_vs_actual.png  # Sample results
â”‚   â”œâ”€â”€ 09_error_distribution.png     # Error analysis
â”‚   â”œâ”€â”€ 10_scatter_pred_vs_actual.png # Correlation plot
â”‚   â”œâ”€â”€ 11_frequency_spectrum_comparison.png # FFT comparison
â”‚   â”œâ”€â”€ 12_long_sequence_predictions.png # Extended series
â”‚   â””â”€â”€ 13_per_frequency_metrics.png  # Performance bars
â”‚
â”œâ”€â”€ ğŸ Python Scripts
â”‚   â”œâ”€â”€ generate_dataset.py           # Step 1: Create data
â”‚   â”œâ”€â”€ visualize_data.py             # Step 2: Data viz
â”‚   â”œâ”€â”€ prepare_training_data.py      # Step 3: Prepare sequences
â”‚   â”œâ”€â”€ train_model.py                # Step 4: Train LSTM
â”‚   â”œâ”€â”€ evaluate_model.py             # Step 5: Test & evaluate
â”‚   â”œâ”€â”€ create_overview.py            # Step 6: Overview viz
â”‚   â””â”€â”€ summary.py                    # Step 7: Print summary
â”‚
â”œâ”€â”€ ğŸ“„ Configuration
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â”œâ”€â”€ run_all.sh                    # Complete pipeline script
â”‚   â””â”€â”€ README.md                     # This file
â”‚
â””â”€â”€ ğŸ”§ Environment
    â”œâ”€â”€ .venv/                        # Virtual environment
    â””â”€â”€ pyproject.toml                # Project metadata
```

---

## ğŸ”§ Technical Details

### Input Format

The model receives sequences with **5 features** at each timestep:

```python
Input shape: (batch_size, sequence_length=50, features=5)

Features:
  [0] â†’ S(x): Combined signal value at time t
  [1] â†’ câ‚: Selector for fâ‚ (1 if selected, else 0)
  [2] â†’ câ‚‚: Selector for fâ‚‚ (1 if selected, else 0)
  [3] â†’ câ‚ƒ: Selector for fâ‚ƒ (1 if selected, else 0)
  [4] â†’ câ‚„: Selector for fâ‚„ (1 if selected, else 0)
```

**Example**: To extract 3 Hz frequency from the mixed signal:
```python
Input = [
  [S(tâ‚€), 0, 1, 0, 0],  # timestep 0: signal + "select fâ‚‚"
  [S(tâ‚), 0, 1, 0, 0],  # timestep 1: signal + "select fâ‚‚"
  ...
  [S(tâ‚„â‚‰), 0, 1, 0, 0] # timestep 49: signal + "select fâ‚‚"
]
```

### Output Format

The model outputs the **filtered frequency** at each timestep:

```python
Output shape: (batch_size, sequence_length=50, 1)

Values: Amplitude of selected frequency fáµ¢(x) at each time t
```

### One-Hot Selector Encoding

| Selector | Binary | Purpose |
|----------|--------|---------|
| **câ‚** | `[1,0,0,0]` | Extract fâ‚ (1.0 Hz) |
| **câ‚‚** | `[0,1,0,0]` | Extract fâ‚‚ (3.0 Hz) |
| **câ‚ƒ** | `[0,0,1,0]` | Extract fâ‚ƒ (5.0 Hz) |
| **câ‚„** | `[0,0,0,1]` | Extract fâ‚„ (7.0 Hz) |

### Loss Function Rationale

We use **Mean Squared Error (MSE)** because:

1. âœ… **Regression task**: Predicting continuous signal amplitudes
2. âœ… **Amplitude matching**: Penalizes large errors more heavily
3. âœ… **Smooth gradients**: Provides stable training signals
4. âœ… **Standard choice**: Proven effective for signal processing
5. âœ… **Interpretable**: MSE in amplitudeÂ² units

Alternative loss functions considered:
- **MAE**: More robust to outliers (used as secondary metric)
- **Huber Loss**: Combines MSE + MAE benefits
- **Custom SNR loss**: Could maximize signal-to-noise ratio

---

## ğŸ’¡ Key Insights

### What We Learned

1. ğŸ¯ **Lower frequencies are easier to extract**
   - fâ‚ (1 Hz, 0Â°): RÂ² = 0.970 (best performance)
   - Longer wavelengths provide more context within 50-timestep windows
   - Phase shift doesn't significantly impact performance

2. ğŸ§  **LSTMs excel at phase-invariant pattern recognition**
   - Successfully learned phase relationships and temporal offsets
   - Maintained coherence across sequence boundaries despite different phases
   - Adapted behavior based on one-hot selector
   - **Phase shifts actually improved generalization** - model learned robust features

3. ğŸ“Š **Model generalizes exceptionally well to phase-shifted signals**
   - No overfitting despite 201K parameters
   - Test performance (RÂ² = 0.945) close to validation (RÂ² = 0.947)
   - Dropout and weight decay were effective
   - **Handles all phase offsets equally well** (0Â° to 135Â°)

4. ğŸ¼ **Frequency separation is learnable in time domain**
   - Model discovered frequency-specific patterns without explicit FFT
   - Works in time domain, unlike traditional filters
   - One-hot encoding provides clear instruction signal
   - **Phase information is implicitly encoded** in LSTM hidden states

5. âš¡ **Real-time filtering is feasible**
   - Fast inference (milliseconds per sequence)
   - Could process streaming audio with sliding windows
   - No need for complex signal processing pipelines
   - **Phase shifts don't slow down inference**

6. ğŸŒŠ **Phase shifts create a more challenging task**
   - Different starting conditions test model robustness
   - More realistic simulation of real-world signals
   - Forces model to learn temporal structure, not just memorize patterns

### Performance Patterns

| Observation | Implication |
|-------------|-------------|
| RÂ² decreases with frequency | Higher frequencies need more samples per cycle |
| Phase shift has minimal impact on RÂ² | Model learned phase-invariant representations |
| Errors are unbiased (mean â‰ˆ 0) | Model is not systematically over/under-predicting |
| Normal error distribution | Prediction uncertainty is well-calibrated |
| Strong validation performance | Hyperparameters are well-tuned |
| Smooth loss curves | Training is stable, no need for adjustments |
| fâ‚ and fâ‚„ perform best | Edge frequencies benefit from less interference |

---

## ğŸ”® Future Improvements

### Potential Extensions

- [ ] **Variable phase shifts**: Random phases Î¸ âˆˆ [0, 2Ï€) for each sample
- [ ] **More frequencies**: Expand to 8-16 frequencies with random phases
- [ ] **Amplitude variations**: Add A_i coefficients to create A_iÂ·sin(2Ï€f_iÂ·x + Î¸_i)
- [ ] **Add noise tolerance**: Test with Gaussian noise, pink noise, and signal corruption
- [ ] **Bidirectional LSTM**: Process sequences in both directions for better accuracy
- [ ] **Attention mechanism**: Let model focus on relevant time steps for each frequency
- [ ] **Multi-frequency selection**: Extract multiple frequencies simultaneously
- [ ] **Non-sinusoidal waveforms**: Test on square waves, triangle waves, sawtooth
- [ ] **Real audio signals**: Apply to actual music/speech frequency filtering
- [ ] **Deeper architectures**: Experiment with 3-4 LSTM layers
- [ ] **Transformer model**: Compare against attention-based architectures
- [ ] **Real-time deployment**: Create web app with live frequency filtering
- [ ] **GPU optimization**: Accelerate training with CUDA
- [ ] **Ensemble methods**: Combine multiple models for robustness

### Research Directions

1. **Adaptive frequency filtering**: Learn to filter arbitrary frequencies (not just 4 fixed ones)
2. **Time-varying frequencies**: Handle chirps and frequency modulation
3. **Phase estimation**: Extract phase information Î¸_i from mixed signals
4. **Multi-channel signals**: Process stereo or multi-sensor data with phase differences
5. **Anomaly detection**: Identify unusual frequency or phase patterns
6. **Compressed representations**: Learn efficient signal encodings

---

## ğŸ“š References & Resources

### Academic Background

- **LSTM Networks**: Hochreiter & Schmidhuber (1997) - ["Long Short-Term Memory"](https://www.bioinf.jku.at/publications/older/2604.pdf)
- **Signal Processing**: Digital signal processing fundamentals
- **Fourier Analysis**: Understanding frequency domain representations

### Technical Documentation

- ğŸ”¥ [PyTorch LSTM Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)
- ğŸ“Š [Time Series Forecasting with Deep Learning](https://pytorch.org/tutorials/beginner/timeseries_tutorial.html)
- ğŸµ [Digital Signal Processing](https://en.wikipedia.org/wiki/Digital_signal_processing)

### Tools Used

| Tool | Purpose | Version |
|------|---------|---------|
| PyTorch | Deep learning framework | 2.0+ |
| NumPy | Numerical computations | 1.24+ |
| Matplotlib | Data visualization | 3.7+ |
| SciPy | Signal processing (FFT) | 1.10+ |
| scikit-learn | Metrics & data splitting | 1.3+ |
| pandas | Data manipulation | 2.0+ |

---

## ğŸ¤ Contributing

Contributions are welcome! Here are some ways you can help:

- ğŸ› Report bugs or issues
- ğŸ’¡ Suggest new features or improvements
- ğŸ“– Improve documentation
- ğŸ§ª Add tests or examples
- ğŸ¨ Create additional visualizations

---

## ğŸ“„ License

MIT License - feel free to use this project for learning, research, or commercial purposes.

---

### Contact & Links

- ğŸ“§ Questions? Open an issue!
- â­ Like this project? Give it a star!
- ğŸ”— [Repository](https://github.com/imraf/lstm-frequency-filter)

---

<div align="center">

### ğŸ‰ Project Achievements

âœ… 10,000 high-quality samples generated  
âœ… 201,345-parameter LSTM trained  
âœ… 94.8% RÂ² score achieved  
âœ… 14 comprehensive visualizations created  
âœ… Real-time frequency filtering demonstrated  

**Thank you for exploring this project!**

*If you found this helpful, please consider starring â­ the repository*

---

**Built with** â¤ï¸ **using PyTorch and Python**

</div>
